{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize mediapipe pose class.\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Create a function to calculate Euclidean distance between two points.\n",
    "def calculate_distance(point1, point2):\n",
    "    return np.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n",
    "\n",
    "# Load the reference image.\n",
    "reference_image = cv2.imread(r'c:\\Users\\Dhruv\\Downloads\\1_frame_779.jpg')\n",
    "\n",
    "# Create a Pose object for reference image processing.\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.3, model_complexity=2)\n",
    "\n",
    "# Process the reference image\n",
    "reference_results = pose.process(cv2.cvtColor(reference_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Retrieve the landmarks from the reference image.\n",
    "reference_landmarks = []\n",
    "if reference_results.pose_landmarks:\n",
    "    for landmark in reference_results.pose_landmarks.landmark:\n",
    "        reference_landmarks.append((landmark.x, landmark.y))\n",
    "\n",
    "# Define the input video file path.\n",
    "input_video_path = r'dataset/videos/icb.mp4'  # Replace with your input video file path.\n",
    "# input_video_path = r'dataset/videos/dataset_video.mp4'\n",
    "# Open the input video file.\n",
    "input_video = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Get video properties.\n",
    "frame_width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(input_video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Create a window to display the output video.\n",
    "cv2.namedWindow(\"Processed Video\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Iterate through the frames in the input video.\n",
    "while True:\n",
    "    ret, frame = input_video.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Process the frame to detect pose landmarks.\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_results = pose.process(frame_rgb)\n",
    "\n",
    "    # Retrieve the landmarks from the video frame.\n",
    "    frame_landmarks = []\n",
    "    if frame_results.pose_landmarks:\n",
    "        for landmark in frame_results.pose_landmarks.landmark:\n",
    "            frame_landmarks.append((landmark.x, landmark.y))\n",
    "\n",
    "    # Create a copy of the frame for drawing.\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    # Draw white lines between keypoints in the reference frame.\n",
    "    for connection in mp_pose.POSE_CONNECTIONS:\n",
    "        start_point = mp_pose.PoseLandmark(connection[0]).value\n",
    "        end_point = mp_pose.PoseLandmark(connection[1]).value\n",
    "        x1, y1 = int(reference_landmarks[start_point][0] * frame.shape[1]), int(reference_landmarks[start_point][1] * frame.shape[0])\n",
    "        x2, y2 = int(reference_landmarks[end_point][0] * frame.shape[1]), int(reference_landmarks[end_point][1] * frame.shape[0])\n",
    "        cv2.line(frame_copy, (x1, y1), (x2, y2), (255, 255, 255), 2)\n",
    "\n",
    "    # Iterate through the keypoints in both reference and frame.\n",
    "    for i in range(len(reference_landmarks)):\n",
    "        if i < len(frame_landmarks):  # Check if there are enough landmarks in the video frame.\n",
    "            reference_point = reference_landmarks[i]\n",
    "            frame_point = frame_landmarks[i]\n",
    "\n",
    "            # Calculate the Euclidean distance between the reference and frame keypoints.\n",
    "            distance = calculate_distance(reference_point, frame_point)\n",
    "\n",
    "            # Define a threshold for matching.\n",
    "            threshold = 0.1  # You can adjust this threshold as needed.\n",
    "\n",
    "            # Check if the distance is below the threshold for matching.\n",
    "            if distance < threshold:\n",
    "                # Draw a green circle for matching keypoints.\n",
    "                cv2.circle(frame_copy, (int(frame_point[0] * frame.shape[1]), int(frame_point[1] * frame.shape[0])), 5, (0, 255, 0), -1)\n",
    "            else:\n",
    "                # Draw a red circle for non-matching keypoints.\n",
    "                cv2.circle(frame_copy, (int(frame_point[0] * frame.shape[1]), int(frame_point[1] * frame.shape[0])), 5, (0, 0, 255), -1)\n",
    "\n",
    "    # Display the modified frame with keypoints and white lines.\n",
    "    cv2.imshow(\"Processed Video\", frame_copy)\n",
    "\n",
    "    # Check for user input to exit the loop.\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the video object.\n",
    "input_video.release()\n",
    "\n",
    "# Close the display window.\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rectangular frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO model for person detection (make sure you have darknet installed).\n",
    "net = cv2.dnn.readNet(r\"yolov4.weights\", r\"yolov4.cfg\")\n",
    "\n",
    "# Set YOLO classes and layer names.\n",
    "classes = [\"person\"]\n",
    "layer_names = net.getUnconnectedOutLayersNames()\n",
    "\n",
    "# Initialize the webcam or another video source.\n",
    "video = cv2.VideoCapture(0)  # 0 represents the default camera, you can change this to your desired video source.\n",
    "\n",
    "# Initialize a window to display the live video feed.\n",
    "cv2.namedWindow(\"YOLO Person Detection\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Iterate through the live video frames.\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform YOLO object detection for persons.\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward(layer_names)\n",
    "\n",
    "    # Initialize a list to track detected persons.\n",
    "    detected_persons = []\n",
    "\n",
    "    # Iterate through the detections.\n",
    "    for detection in detections:\n",
    "        for obj in detection:\n",
    "            scores = obj[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if class_id == 0 and confidence > 0.5:  # \"0\" corresponds to the \"person\" class.\n",
    "                # Extract bounding box coordinates.\n",
    "                center_x, center_y, width, height = map(int, obj[0:4] * np.array([frame.shape[1], frame.shape[0], frame.shape[1], frame.shape[0]]))\n",
    "                x, y = int(center_x - width / 2), int(center_y - height / 2)\n",
    "\n",
    "                # Append the detected person to the list.\n",
    "                detected_persons.append((x, y, width, height))\n",
    "\n",
    "    # Track the first detected person.\n",
    "    if detected_persons:\n",
    "        x, y, width, height = detected_persons[0]\n",
    "        cv2.rectangle(frame, (x, y), (x + width, y + height), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with detected persons.\n",
    "    cv2.imshow(\"YOLO Person Detection\", frame)\n",
    "\n",
    "    # Check for user input to exit the loop.\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the video source and close the window.\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dynamic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize mediapipe pose class.\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Create a function to calculate Euclidean distance between two points.\n",
    "def calculate_distance(point1, point2):\n",
    "    return np.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n",
    "\n",
    "# Load the reference image.\n",
    "reference_image = cv2.imread(r'c:\\Users\\Dhruv\\Downloads\\1_frame_779.jpg')\n",
    "\n",
    "# Create a Pose object for reference image processing.\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.3, model_complexity=2)\n",
    "\n",
    "# Process the reference image.\n",
    "reference_results = pose.process(cv2.cvtColor(reference_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Retrieve the landmarks from the reference image.\n",
    "reference_landmarks = []\n",
    "if reference_results.pose_landmarks:\n",
    "    for landmark in reference_results.pose_landmarks.landmark:\n",
    "        reference_landmarks.append((landmark.x, landmark.y))\n",
    "\n",
    "# Define the input video file path.\n",
    "input_video_path = r'dataset/videos/ice.mp4'\n",
    "\n",
    "# Open the input video file.\n",
    "input_video = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Iterate through the frames in the input video.\n",
    "while True:\n",
    "    ret, frame = input_video.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Process the frame to detect pose landmarks.\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_results = pose.process(frame_rgb)\n",
    "\n",
    "    # Retrieve the landmarks from the video frame.\n",
    "    frame_landmarks = []\n",
    "    if frame_results.pose_landmarks:\n",
    "        for landmark in frame_results.pose_landmarks.landmark:\n",
    "            frame_landmarks.append((landmark.x, landmark.y))\n",
    "\n",
    "    # Calculate the initial offset between reference and detected keypoints.\n",
    "    if len(frame_landmarks) > 0 and len(reference_landmarks) > 0:\n",
    "        initial_offset = np.array(frame_landmarks[0]) - np.array(reference_landmarks[0])\n",
    "\n",
    "    # Create a copy of the frame for drawing.\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    # Iterate through the keypoints in both reference and frame, adjusting for the offset.\n",
    "    for i in range(len(reference_landmarks)):\n",
    "        if i < len(frame_landmarks):\n",
    "            reference_point = reference_landmarks[i]\n",
    "            adjusted_frame_point = (frame_landmarks[i][0] - initial_offset[0], frame_landmarks[i][1] - initial_offset[1])\n",
    "\n",
    "            # Calculate the Euclidean distance between adjusted keypoints.\n",
    "            distance = calculate_distance(reference_point, adjusted_frame_point)\n",
    "\n",
    "            # Define a threshold for matching.\n",
    "            threshold = 0.1\n",
    "\n",
    "            # Check if the distance is below the threshold for matching.\n",
    "            if distance < threshold:\n",
    "                cv2.circle(frame_copy, (int(adjusted_frame_point[0] * frame.shape[1]), int(adjusted_frame_point[1] * frame.shape[0])), 5, (0, 255, 0), -1)\n",
    "            else:\n",
    "                cv2.circle(frame_copy, (int(adjusted_frame_point[0] * frame.shape[1]), int(adjusted_frame_point[1] * frame.shape[0])), 5, (0, 0, 255), -1)\n",
    "\n",
    "    # Display the modified frame with keypoints.\n",
    "    cv2.imshow(\"Processed Video\", frame_copy)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the video object.\n",
    "input_video.release()\n",
    "\n",
    "# Close the display window.\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize mediapipe pose class.\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Create a function to calculate Euclidean distance between two points.\n",
    "def calculate_distance(point1, point2):\n",
    "    return np.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n",
    "\n",
    "# Load the reference image.\n",
    "# reference_image = cv2.imread(r'c:\\Users\\Dhruv\\Downloads\\1_frame_779.jpg')\n",
    "reference_image = cv2.imread(r'dataset/amit_reference.jpg')\n",
    "\n",
    "# Create a Pose object for reference image processing.\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.3, model_complexity=2)\n",
    "\n",
    "# Process the reference image.\n",
    "reference_results = pose.process(cv2.cvtColor(reference_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Retrieve the landmarks from the reference image.\n",
    "reference_landmarks = []\n",
    "if reference_results.pose_landmarks:\n",
    "    for landmark in reference_results.pose_landmarks.landmark:\n",
    "        reference_landmarks.append((landmark.x, landmark.y))\n",
    "\n",
    "# Define the input video file path.\n",
    "input_video_path = r'dataset/test.mp4'\n",
    "\n",
    "# Open the input video file.\n",
    "input_video = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Iterate through the frames in the input video.\n",
    "while True:\n",
    "    ret, frame = input_video.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Process the frame to detect pose landmarks.\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_results = pose.process(frame_rgb)\n",
    "\n",
    "    # Retrieve the landmarks from the video frame.\n",
    "    frame_landmarks = []\n",
    "    if frame_results.pose_landmarks:\n",
    "        for landmark in frame_results.pose_landmarks.landmark:\n",
    "            frame_landmarks.append((landmark.x, landmark.y))\n",
    "\n",
    "    # Calculate the initial offset between reference and detected keypoints.\n",
    "    if len(frame_landmarks) > 0 and len(reference_landmarks) > 0:\n",
    "        initial_offset = np.array(reference_landmarks[0]) - np.array(frame_landmarks[0])\n",
    "\n",
    "    # Create a copy of the frame for drawing.\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    # Iterate through the keypoints in both reference and frame, adjusting for the offset.\n",
    "    for i in range(len(frame_landmarks)):\n",
    "        if i < len(reference_landmarks):\n",
    "            frame_point = frame_landmarks[i]\n",
    "            adjusted_reference_point = (reference_landmarks[i][0] - initial_offset[0], reference_landmarks[i][1] - initial_offset[1])\n",
    "\n",
    "            # Calculate the Euclidean distance between adjusted keypoints.\n",
    "            distance = calculate_distance(adjusted_reference_point, frame_point)\n",
    "\n",
    "            # Define a threshold for matching.\n",
    "            threshold = 0.1\n",
    "\n",
    "            # Check if the distance is below the threshold for matching.\n",
    "            if distance < threshold:\n",
    "                cv2.circle(frame_copy, (int(frame_point[0] * frame.shape[1]), int(frame_point[1] * frame.shape[0])), 5, (0, 255, 0), -1)\n",
    "            else:\n",
    "                cv2.circle(frame_copy, (int(frame_point[0] * frame.shape[1]), int(frame_point[1] * frame.shape[0])), 5, (0, 0, 255), -1)\n",
    "\n",
    "    # Display the modified frame with keypoints.\n",
    "    cv2.imshow(\"Processed Video\", frame_copy)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the video object.\n",
    "input_video.release()\n",
    "\n",
    "# Close the display window.\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize mediapipe pose class.\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Create a function to calculate Euclidean distance between two points.\n",
    "def calculate_distance(point1, point2):\n",
    "    return np.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n",
    "\n",
    "# Load the reference image.\n",
    "reference_image = cv2.imread(r'c:\\Users\\Dhruv\\Downloads\\1_frame_779.jpg')\n",
    "\n",
    "# Create a Pose object for reference image processing.\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.3, model_complexity=2)\n",
    "\n",
    "# Process the reference image.\n",
    "reference_results = pose.process(cv2.cvtColor(reference_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Retrieve the landmarks from the reference image.\n",
    "reference_landmarks = []\n",
    "if reference_results.pose_landmarks:\n",
    "    for i, landmark in enumerate(reference_results.pose_landmarks.landmark):\n",
    "        reference_landmarks.append((landmark.x, landmark.y, i))\n",
    "\n",
    "# Define the input video file path.\n",
    "input_video_path = r'dataset/videos/ice.mp4'\n",
    "\n",
    "# Open the input video file.\n",
    "input_video = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Define a dictionary to map keypoint numbers to labels.\n",
    "keypoint_labels = {\n",
    "    \"Perfect\": \"Perfect! Keep Going :)\",\n",
    "    \"HandsNotAtRightPosition\": \"Hands not at right position\",\n",
    "    \"HandsNotAt90\": \"Hands not at 90 degree\",\n",
    "    \"LegsNotTriangle\": \"Legs not triangle\",\n",
    "    \"UnknownErrors\": \"Unknown errors\",\n",
    "    \"LegDown\": \"Leg Down\",\n",
    "    \"Idle\": \"idle, please perform asana\",\n",
    "    \"BentRight\": \"bent right, straiten yourself\",\n",
    "    \"BentLeft\": \"bent left, staiten yourself\",\n",
    "    \"BentForward\": \"bent forward, straiten yourself\"\n",
    "}\n",
    "\n",
    "# Iterate through the frames in the input video.\n",
    "while True:\n",
    "    ret, frame = input_video.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Process the frame to detect pose landmarks.\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_results = pose.process(frame_rgb)\n",
    "\n",
    "    # Retrieve the landmarks from the video frame.\n",
    "    frame_landmarks = []\n",
    "    if frame_results.pose_landmarks:\n",
    "        for i, landmark in enumerate(frame_results.pose_landmarks.landmark):\n",
    "            frame_landmarks.append((landmark.x, landmark.y, i))\n",
    "\n",
    "    # Calculate the initial offset between reference and detected keypoints.\n",
    "    if len(frame_landmarks) > 0 and len(reference_landmarks) > 0:\n",
    "        initial_offset = np.array(reference_landmarks[0][:2]) - np.array(frame_landmarks[0][:2])\n",
    "\n",
    "    # Create a copy of the frame for drawing.\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    # Create flags to track the conditions.\n",
    "    all_green = True\n",
    "    hands_not_joined = False\n",
    "    hands_not_at_90 = False\n",
    "    legs_not_triangle = False\n",
    "\n",
    "    # Iterate through the keypoints in both reference and frame, adjusting for the offset.\n",
    "    for frame_landmark in frame_landmarks:\n",
    "        frame_point = frame_landmark[:2]\n",
    "        frame_keypoint_number = frame_landmark[2]\n",
    "        adjusted_reference_point = (reference_landmarks[frame_keypoint_number][0] - initial_offset[0], reference_landmarks[frame_keypoint_number][1] - initial_offset[1])\n",
    "\n",
    "        # Calculate the Euclidean distance between adjusted keypoints.\n",
    "        distance = calculate_distance(adjusted_reference_point, frame_point)\n",
    "\n",
    "        # Define a threshold for matching.\n",
    "        threshold = 0.1\n",
    "\n",
    "        # Check if the distance is below the threshold for matching.\n",
    "        if distance < threshold:\n",
    "            cv2.circle(frame_copy, (int(frame_point[0] * frame.shape[1]), int(frame_point[1] * frame.shape[0])), 5, (0, 255, 0), -1)\n",
    "        else:\n",
    "            all_green = False\n",
    "            if frame_keypoint_number in [12, 13]:\n",
    "                hands_not_at_90 = True\n",
    "            if frame_keypoint_number in [18, 20, 16] or frame_keypoint_number in [17, 19, 21]:\n",
    "                hands_not_joined = True\n",
    "            if frame_keypoint_number in [24, 25]:\n",
    "                legs_not_triangle = True\n",
    "\n",
    "            cv2.circle(frame_copy, (int(frame_point[0] * frame.shape[1]), int(frame_point[1] * frame.shape[0])), 5, (0, 0, 255), -1)  # Red for specific keypoints\n",
    "\n",
    "    # Determine the label based on the conditions.\n",
    "    if all_green:\n",
    "        label = keypoint_labels[\"Perfect\"]\n",
    "    else:\n",
    "        error_labels = []\n",
    "\n",
    "        if hands_not_at_90:\n",
    "            error_labels.append(keypoint_labels[\"HandsNotAt90\"])\n",
    "        if hands_not_joined:\n",
    "            error_labels.append(keypoint_labels[\"HandsNotAtRightPosition\"])\n",
    "        if legs_not_triangle:\n",
    "            error_labels.append(keypoint_labels[\"LegsNotTriangle\"])\n",
    "\n",
    "        if not error_labels:\n",
    "            label = keypoint_labels[\"UnknownErrors\"]\n",
    "        else:\n",
    "            label = \"\\n\".join(error_labels)  # Separate labels on different lines\n",
    "\n",
    "    # Display the label on the frame.\n",
    "    label_lines = label.split('\\n')\n",
    "    for i, line in enumerate(label_lines):\n",
    "        cv2.putText(frame_copy, line, (10, 30 + i * 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Display the modified frame with keypoints and label.\n",
    "    cv2.imshow(\"Processed Video\", frame_copy)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the video object.\n",
    "input_video.release()\n",
    "\n",
    "# Close the display window.\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detailed correction - bent left right forward galat hai bas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize mediapipe pose class.\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Create a function to calculate Euclidean distance between two points.\n",
    "def calculate_distance(point1, point2):\n",
    "    return np.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n",
    "\n",
    "# Load the reference image.\n",
    "reference_image = cv2.imread(r'dataset/amit_reference.jpg')\n",
    "\n",
    "# Create a Pose object for reference image processing.\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.3, model_complexity=2)\n",
    "\n",
    "# Process the reference image.\n",
    "reference_results = pose.process(cv2.cvtColor(reference_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Retrieve the landmarks from the reference image.\n",
    "reference_landmarks = []\n",
    "if reference_results.pose_landmarks:\n",
    "    for i, landmark in enumerate(reference_results.pose_landmarks.landmark):\n",
    "        reference_landmarks.append((landmark.x, landmark.y, i))\n",
    "\n",
    "# ********************************************************************************************************************\n",
    "# input_video_path = r'dataset/videos/pcb.mp4'\n",
    "# input_video_path = r'dataset/videos/icb.mp4'\n",
    "# input_video_path = r'c:\\Users\\Dhruv\\Downloads\\WhatsApp Video 2023-08-22 at 18.04.54.mp4'\n",
    "input_video_path = r'dataset/test.mp4'\n",
    "\n",
    "# Open the input video file.\n",
    "input_video = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Define a dictionary to map keypoint numbers to labels.\n",
    "keypoint_labels = {\n",
    "    \"Perfect\": \"Perfect! Keep Going :)\",\n",
    "    \"HandsNotAtRightPosition\": \"Hands not at right position\",\n",
    "    \"HandsNotAt90\": \"Hands not at 90 degree\",\n",
    "    \"LegsNotTriangle\": \"Legs not triangle\",\n",
    "    \"Hands_legs_wrong\": \"hands and leg both wrong\",\n",
    "    \"LegDown\": \"Leg Down\",\n",
    "    \"Idle\": \"Idle, please perform asana\",\n",
    "    \"BentRight\": \"bent right, straiten yourself\",\n",
    "    \"BentLeft\": \"bent left, staiten yourself\",\n",
    "    \"BentForward\": \"bent forward, straiten yourself\"\n",
    "}\n",
    "\n",
    "# Iterate through the frames in the input video.\n",
    "while True:\n",
    "    ret, frame = input_video.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Process the frame to detect pose landmarks.\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_results = pose.process(frame_rgb)\n",
    "\n",
    "    # Retrieve the landmarks from the video frame.\n",
    "    frame_landmarks = []\n",
    "    if frame_results.pose_landmarks:\n",
    "        for i, landmark in enumerate(frame_results.pose_landmarks.landmark):\n",
    "            frame_landmarks.append((landmark.x, landmark.y, i))\n",
    "\n",
    "    # Calculate the initial offset between reference and detected keypoints.\n",
    "    if len(frame_landmarks) > 0 and len(reference_landmarks) > 0:\n",
    "        initial_offset = np.array(reference_landmarks[0][:2]) - np.array(frame_landmarks[0][:2])\n",
    "\n",
    "    # Create a copy of the frame for drawing.\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    # Create flags to track the conditions.\n",
    "    leg_down = False\n",
    "    hands_not_at_right_position = False\n",
    "    hands_not_at_90 = False\n",
    "    bent_right = False\n",
    "    bent_left = False\n",
    "\n",
    "    # Iterate through the keypoints in both reference and frame, adjusting for the offset.\n",
    "    for frame_landmark in frame_landmarks:\n",
    "        frame_point = frame_landmark[:2]\n",
    "        frame_keypoint_number = frame_landmark[2]\n",
    "        adjusted_reference_point = (reference_landmarks[frame_keypoint_number][0] - initial_offset[0], reference_landmarks[frame_keypoint_number][1] - initial_offset[1])\n",
    "\n",
    "        # Calculate the Euclidean distance between adjusted keypoints.\n",
    "        distance = calculate_distance(adjusted_reference_point, frame_point)\n",
    "\n",
    "        # Define a threshold for matching.\n",
    "        threshold = 0.1\n",
    "\n",
    "        # Check if the distance is below the threshold for matching.\n",
    "        if distance < threshold:\n",
    "            cv2.circle(frame_copy, (int(frame_point[0] * frame.shape[1]), int(frame_point[1] * frame.shape[0])), 5, (0, 255, 0), -1)\n",
    "        else:\n",
    "            if frame_keypoint_number in [26, 28, 30] or frame_keypoint_number in [27, 29, 31] or frame_keypoint_number == 24 or frame_keypoint_number == 25:\n",
    "                leg_down = True\n",
    "            if frame_keypoint_number == 12 or frame_keypoint_number == 13:\n",
    "                hands_not_at_90 = True\n",
    "            if frame_keypoint_number in [18, 20, 16] or frame_keypoint_number in [17, 19, 21]:\n",
    "                hands_not_at_right_position = True\n",
    "            if frame_keypoint_number in [1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "                if frame_point[0] < adjusted_reference_point[0]:\n",
    "                    bent_right = True\n",
    "                elif frame_point[0] > adjusted_reference_point[0]:\n",
    "                    bent_left = True\n",
    "\n",
    "            cv2.circle(frame_copy, (int(frame_point[0] * frame.shape[1]), int(frame_point[1] * frame.shape[0])), 5, (0, 0, 255), -1)  # Red for specific keypoints\n",
    "\n",
    "    # Determine the label based on the conditions.\n",
    "    if leg_down:\n",
    "        if hands_not_at_right_position:\n",
    "            if not hands_not_at_90:\n",
    "                label = keypoint_labels[\"Idle\"]\n",
    "            else:\n",
    "                label = keypoint_labels[\"Hands_legs_wrong\"]\n",
    "        else:\n",
    "            label = keypoint_labels[\"LegDown\"]\n",
    "    elif bent_right:\n",
    "        label = keypoint_labels[\"BentRight\"]\n",
    "    elif bent_left:\n",
    "        label = keypoint_labels[\"BentLeft\"]\n",
    "    elif hands_not_at_90:\n",
    "        label = keypoint_labels[\"HandsNotAt90\"]\n",
    "    elif hands_not_at_right_position:\n",
    "        label = keypoint_labels[\"HandsNotAtRightPosition\"]\n",
    "    else:\n",
    "        label = keypoint_labels[\"Perfect\"]\n",
    "\n",
    "    # Display the label on the frame.\n",
    "    label_lines = label.split('\\n')\n",
    "    for i, line in enumerate(label_lines):\n",
    "        cv2.putText(frame_copy, line, (10, 30 + i * 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Display the modified frame with keypoints and label.\n",
    "    cv2.imshow(\"Processed Video\", frame_copy)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the video object.\n",
    "input_video.release()\n",
    "\n",
    "# Close the display window.\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize mediapipe pose class.\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Create a function to calculate Euclidean distance between two points.\n",
    "def calculate_distance(point1, point2):\n",
    "    return np.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n",
    "\n",
    "# Load the reference image.\n",
    "reference_image = cv2.imread(r'dataset/amit_reference.jpg')\n",
    "\n",
    "# Create a Pose object for reference image processing.\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.3, model_complexity=2)\n",
    "\n",
    "# Process the reference image.\n",
    "reference_results = pose.process(cv2.cvtColor(reference_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Retrieve the landmarks from the reference image.\n",
    "reference_landmarks = []\n",
    "if reference_results.pose_landmarks:\n",
    "    for i, landmark in enumerate(reference_results.pose_landmarks.landmark):\n",
    "        reference_landmarks.append((landmark.x, landmark.y, i))\n",
    "\n",
    "# Define the input video path.\n",
    "input_video_path = r'dataset/test.mp4'\n",
    "\n",
    "# Open the input video file.\n",
    "input_video = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Define the output video parameters.\n",
    "output_video_path = 'output_video.mp4'  # Choose your desired output file name\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fps = int(input_video.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Create a VideoWriter object to save the output video.\n",
    "output_video = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Define a dictionary to map keypoint numbers to labels.\n",
    "keypoint_labels = {\n",
    "    \"Perfect\": \"Perfect! Keep Going :)\",\n",
    "    \"HandsNotAtRightPosition\": \"Hands not at the right \\nposition\",\n",
    "    \"HandsNotAt90\": \"Hands not at 90 degrees\",\n",
    "    \"LegsNotTriangle\": \"Legs not in a triangle shape\",\n",
    "    \"Hands_legs_wrong\": \"Hands and legs both at \\nwrong positions\",\n",
    "    \"LegDown\": \"Leg Down\",\n",
    "    \"Idle\": \"Idle, please perform the \\nasana\",\n",
    "    \"BentRight\": \"Bent to the right, straighten yourself\",\n",
    "    \"BentLeft\": \"Bent to the left, straighten yourself\",\n",
    "    \"BentForward\": \"Bent forward, straighten yourself\"\n",
    "}\n",
    "\n",
    "# Iterate through the frames in the input video.\n",
    "while True:\n",
    "    ret, frame = input_video.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Process the frame to detect pose landmarks.\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_results = pose.process(frame_rgb)\n",
    "\n",
    "    # Retrieve the landmarks from the video frame.\n",
    "    frame_landmarks = []\n",
    "    if frame_results.pose_landmarks:\n",
    "        for i, landmark in enumerate(frame_results.pose_landmarks.landmark):\n",
    "            frame_landmarks.append((landmark.x, landmark.y, i))\n",
    "\n",
    "    # Calculate the initial offset between reference and detected keypoints.\n",
    "    if len(frame_landmarks) > 0 and len(reference_landmarks) > 0:\n",
    "        initial_offset = np.array(reference_landmarks[0][:2]) - np.array(frame_landmarks[0][:2])\n",
    "\n",
    "    # Create a copy of the frame for drawing.\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    # Create flags to track the conditions.\n",
    "    leg_down = False\n",
    "    hands_not_at_right_position = False\n",
    "    hands_not_at_90 = False\n",
    "    bent_right = False\n",
    "    bent_left = False\n",
    "\n",
    "    # Iterate through the keypoints in both reference and frame, adjusting for the offset.\n",
    "    for frame_landmark in frame_landmarks:\n",
    "        frame_point = frame_landmark[:2]\n",
    "        frame_keypoint_number = frame_landmark[2]\n",
    "        adjusted_reference_point = (reference_landmarks[frame_keypoint_number][0] - initial_offset[0], reference_landmarks[frame_keypoint_number][1] - initial_offset[1])\n",
    "\n",
    "        # Calculate the Euclidean distance between adjusted keypoints.\n",
    "        distance = calculate_distance(adjusted_reference_point, frame_point)\n",
    "\n",
    "        # Define a threshold for matching.\n",
    "        threshold = 0.1\n",
    "\n",
    "        # Check if the distance is below the threshold for matching.\n",
    "        if distance < threshold:\n",
    "            cv2.circle(frame_copy, (int(frame_point[0] * frame.shape[1]), int(frame_point[1] * frame.shape[0])), 5, (0, 255, 0), -1)\n",
    "        else:\n",
    "            if frame_keypoint_number in [26, 28, 30] or frame_keypoint_number in [27, 29, 31] or frame_keypoint_number == 24 or frame_keypoint_number == 25:\n",
    "                leg_down = True\n",
    "            if frame_keypoint_number == 12 or frame_keypoint_number == 13:\n",
    "                hands_not_at_90 = True\n",
    "            if frame_keypoint_number in [18, 20, 16] or frame_keypoint_number in [17, 19, 21]:\n",
    "                hands_not_at_right_position = True\n",
    "            if frame_keypoint_number in [1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "                if frame_point[0] < adjusted_reference_point[0]:\n",
    "                    bent_right = True\n",
    "                elif frame_point[0] > adjusted_reference_point[0]:\n",
    "                    bent_left = True\n",
    "\n",
    "            cv2.circle(frame_copy, (int(frame_point[0] * frame.shape[1]), int(frame_point[1] * frame.shape[0])), 5, (0, 0, 255), -1)  # Red for specific keypoints\n",
    "\n",
    "    # Determine the label based on the conditions.\n",
    "    if leg_down:\n",
    "        if hands_not_at_right_position:\n",
    "            if not hands_not_at_90:\n",
    "                label = keypoint_labels[\"Idle\"]\n",
    "            else:\n",
    "                label = keypoint_labels[\"Hands_legs_wrong\"]\n",
    "        else:\n",
    "            label = keypoint_labels[\"LegDown\"]\n",
    "    elif bent_right:\n",
    "        label = keypoint_labels[\"BentRight\"]\n",
    "    elif bent_left:\n",
    "        label = keypoint_labels[\"BentLeft\"]\n",
    "    elif hands_not_at_90:\n",
    "        label = keypoint_labels[\"HandsNotAt90\"]\n",
    "    elif hands_not_at_right_position:\n",
    "        label = keypoint_labels[\"HandsNotAtRightPosition\"]\n",
    "    else:\n",
    "        label = keypoint_labels[\"Perfect\"]\n",
    "\n",
    "    # Display the label on the frame.\n",
    "    label_lines = label.split('\\n')\n",
    "    for i, line in enumerate(label_lines):\n",
    "        cv2.putText(frame_copy, line, (10, 30 + i * 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Write the processed frame to the output video.\n",
    "    output_video.write(frame_copy)\n",
    "\n",
    "    cv2.imshow(\"Processed Video\", frame_copy)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the video objects.\n",
    "input_video.release()\n",
    "output_video.release()\n",
    "\n",
    "# Close the display window.\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bent left and right sahi se chalane wala code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def calculate_distance(point1, point2):\n",
    "    return np.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n",
    "\n",
    "reference_image = cv2.imread(r'c:\\Users\\Dhruv\\Downloads\\1_frame_779.jpg')\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.3, model_complexity=2)\n",
    "reference_results = pose.process(cv2.cvtColor(reference_image, cv2.COLOR_BGR2RGB))\n",
    "reference_landmarks = []\n",
    "if reference_results.pose_landmarks:\n",
    "    for i, landmark in enumerate(reference_results.pose_landmarks.landmark):\n",
    "        reference_landmarks.append((landmark.x, landmark.y, i))\n",
    "\n",
    "input_video_path = r'dataset/videos/pcd.mp4'\n",
    "input_video = cv2.VideoCapture(input_video_path)\n",
    "keypoint_labels = {\n",
    "    \"Perfect\": \"Perfect! Keep Going :)\",\n",
    "    \"HandsNotAtRightPosition\": \"Hands not at right position\",\n",
    "    \"HandsNotAt90\": \"Hands not at 90 degree\",\n",
    "    \"LegsNotTriangle\": \"Legs not triangle\",\n",
    "    \"Hands_legs_wrong\": \"Hands and leg both wrong\",\n",
    "    \"LegDown\": \"Leg Down\",\n",
    "    \"Idle\": \"Idle, please perform asana\",\n",
    "    \"BentRight\": \"Bent right, straighten yourself\",\n",
    "    \"BentLeft\": \"Bent left, straighten yourself\",\n",
    "    \"BentForward\": \"Bent forward, straighten yourself\"\n",
    "}\n",
    "\n",
    "face_keypoints_threshold = 0.2\n",
    "\n",
    "while True:\n",
    "    ret, frame = input_video.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_results = pose.process(frame_rgb)\n",
    "\n",
    "    frame_landmarks = []\n",
    "    if frame_results.pose_landmarks:\n",
    "        for i, landmark in enumerate(frame_results.pose_landmarks.landmark):\n",
    "            frame_landmarks.append((landmark.x, landmark.y, i))\n",
    "\n",
    "    if len(frame_landmarks) > 0 and len(reference_landmarks) > 0:\n",
    "        initial_offset = np.array(reference_landmarks[0][:2]) - np.array(frame_landmarks[0][:2])\n",
    "\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    leg_down = False\n",
    "    hands_not_at_right_position = False\n",
    "    hands_not_at_90 = False\n",
    "    bent_right = False\n",
    "    bent_left = False\n",
    "    bent_forward = False\n",
    "\n",
    "    right_shoulder = None\n",
    "    left_shoulder = None\n",
    "    for frame_landmark in frame_landmarks:\n",
    "        frame_keypoint_number = frame_landmark[2]\n",
    "        if frame_keypoint_number == 10:\n",
    "            left_shoulder = frame_landmark[:2]\n",
    "        elif frame_keypoint_number == 11:\n",
    "            right_shoulder = frame_landmark[:2]\n",
    "\n",
    "    for frame_landmark in frame_landmarks:\n",
    "        frame_point = frame_landmark[:2]\n",
    "        frame_keypoint_number = frame_landmark[2]\n",
    "        adjusted_reference_point = (reference_landmarks[frame_keypoint_number][0] - initial_offset[0], reference_landmarks[frame_keypoint_number][1] - initial_offset[1])\n",
    "        distance = calculate_distance(adjusted_reference_point, frame_point)\n",
    "        threshold = 0.1\n",
    "\n",
    "        if frame_keypoint_number in [1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "            if distance < face_keypoints_threshold:\n",
    "                cv2.circle(frame_copy, (int(frame_point[0] * frame.shape[1]), int(frame_point[1] * frame.shape[0])), 5, (0, 255, 0), -1)\n",
    "            else:\n",
    "                bent_forward = True\n",
    "                cv2.circle(frame_copy, (int(frame_point[0] * frame.shape[1]), int(frame_point[1] * frame.shape[0])), 5, (0, 0, 255), -1)\n",
    "        elif distance < threshold:\n",
    "            cv2.circle(frame_copy, (int(frame_point[0] * frame.shape[1]), int(frame_point[1] * frame.shape[0])), 5, (0, 255, 0), -1)\n",
    "        else:\n",
    "            if frame_keypoint_number in [26, 28, 30] or frame_keypoint_number in [27, 29, 31] or frame_keypoint_number == 24 or frame_keypoint_number == 25:\n",
    "                leg_down = True\n",
    "            if frame_keypoint_number == 12 or frame_keypoint_number == 13:\n",
    "                hands_not_at_90 = True\n",
    "            if frame_keypoint_number in [18, 20, 16] or frame_keypoint_number in [17, 19, 21]:\n",
    "                hands_not_at_right_position = True\n",
    "            cv2.circle(frame_copy, (int(frame_point[0] * frame.shape[1]), int(frame_point[1] * frame.shape[0])), 5, (0, 0, 255), -1)\n",
    "\n",
    "    if bent_left or (right_shoulder and left_shoulder and right_shoulder[1] > left_shoulder[1]):\n",
    "        label = keypoint_labels[\"BentLeft\"]\n",
    "    elif bent_right or (right_shoulder and left_shoulder and right_shoulder[1] < left_shoulder[1]):\n",
    "        label = keypoint_labels[\"BentRight\"]\n",
    "    elif leg_down:\n",
    "        if hands_not_at_right_position:\n",
    "            if not hands_not_at_90:\n",
    "                label = keypoint_labels[\"Idle\"]\n",
    "            else:\n",
    "                label = keypoint_labels[\"Hands_legs_wrong\"]\n",
    "        else:\n",
    "            label = keypoint_labels[\"LegDown\"]\n",
    "    else:\n",
    "        label = keypoint_labels[\"Perfect\"]\n",
    "\n",
    "    label_lines = label.split('\\n')\n",
    "    for i, line in enumerate(label_lines):\n",
    "        cv2.putText(frame_copy, line, (10, 30 + i * 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Processed Video\", frame_copy)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "input_video.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a6255c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFViTForImageClassification.\n",
      "\n",
      "All the layers of TFViTForImageClassification were initialized from the model checkpoint at google/vit-base-patch16-224.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTForImageClassification for predictions without further training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or jax.ndarray, but got <class 'keras.engine.keras_tensor.KerasTensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39m# Add custom classification head\u001b[39;00m\n\u001b[0;32m     22\u001b[0m inputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(IMG_WIDTH, IMG_HEIGHT, \u001b[39m3\u001b[39m))\n\u001b[1;32m---> 23\u001b[0m input_ids \u001b[39m=\u001b[39m feature_extractor(images\u001b[39m=\u001b[39;49minputs)[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     24\u001b[0m outputs \u001b[39m=\u001b[39m base_model(input_ids)[\u001b[39m'\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     25\u001b[0m outputs \u001b[39m=\u001b[39m Flatten()(outputs)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\image_processing_utils.py:494\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[1;34m(self, images, **kwargs)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchFeature:\n\u001b[0;32m    493\u001b[0m     \u001b[39m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 494\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\vit\\image_processing_vit.py:244\u001b[0m, in \u001b[0;36mViTImageProcessor.preprocess\u001b[1;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m size \u001b[39m=\u001b[39m size \u001b[39mif\u001b[39;00m size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize\n\u001b[0;32m    242\u001b[0m size_dict \u001b[39m=\u001b[39m get_size_dict(size)\n\u001b[1;32m--> 244\u001b[0m images \u001b[39m=\u001b[39m make_list_of_images(images)\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid_images(images):\n\u001b[0;32m    247\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    248\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor, tf.Tensor or jax.ndarray.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\image_utils.py:132\u001b[0m, in \u001b[0;36mmake_list_of_images\u001b[1;34m(images, expected_ndims)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    128\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid image shape. Expected either \u001b[39m\u001b[39m{\u001b[39;00mexpected_ndims \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m or \u001b[39m\u001b[39m{\u001b[39;00mexpected_ndims\u001b[39m}\u001b[39;00m\u001b[39m dimensions, but got\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    129\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mimages\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m dimensions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    130\u001b[0m         )\n\u001b[0;32m    131\u001b[0m     \u001b[39mreturn\u001b[39;00m images\n\u001b[1;32m--> 132\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    133\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mInvalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mjax.ndarray, but got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(images)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or jax.ndarray, but got <class 'keras.engine.keras_tensor.KerasTensor'>."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import ViTFeatureExtractor, TFViTForImageClassification\n",
    "\n",
    "# Define constants\n",
    "NUM_CLASSES = 4  # Number of classes (correct, partially correct, incorrect, none)\n",
    "IMG_WIDTH, IMG_HEIGHT = 150, 150  # Input image dimensions\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "\n",
    "# Load and preprocess your dataset here\n",
    "\n",
    "# Load ViT Feature Extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Load the pre-trained Vision Transformer model\n",
    "base_model = TFViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Add custom classification head\n",
    "inputs = tf.keras.Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "input_ids = feature_extractor(images=inputs)['input_ids']\n",
    "outputs = base_model(input_ids)['logits']\n",
    "outputs = Flatten()(outputs)\n",
    "outputs = Dense(1024, activation='relu')(outputs)\n",
    "outputs = Dense(512, activation='relu')(outputs)\n",
    "outputs = Dense(256, activation='relu')(outputs)\n",
    "outputs = Dense(128, activation='relu')(outputs)\n",
    "outputs = Dense(NUM_CLASSES, activation='softmax')(outputs)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create data generators for training and validation\n",
    "TRAINING_DIR = r\"C:/Users/Lenovo/Desktop/koi bhi/train/\"\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAINING_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "VALIDATION_DIR = r\"C:/Users/Lenovo/Desktop/koi bhi/test/\"\n",
    "validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    VALIDATION_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.n // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.n // BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Save the model for later use\n",
    "model.save('vision_transformer_image_classifier.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ac1acee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (9.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pillow numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "397f2ffb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/your/image.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m# Load and preprocess your dataset here\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m# Replace the following with your image loading and preprocessing code\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# Example: Load a single image and preprocess it\u001b[39;00m\n\u001b[0;32m     18\u001b[0m image_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpath/to/your/image.jpg\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 19\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(image_path)\u001b[39m.\u001b[39mresize((IMG_WIDTH, IMG_HEIGHT))\n\u001b[0;32m     20\u001b[0m image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(image) \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m\n\u001b[0;32m     21\u001b[0m image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(image, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:3068\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3065\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   3067\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> 3068\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   3069\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   3071\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/your/image.jpg'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import ViTFeatureExtractor, TFViTForImageClassification\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Define constants\n",
    "NUM_CLASSES = 4  # Number of classes (correct, partially correct, incorrect, none)\n",
    "IMG_WIDTH, IMG_HEIGHT = 150, 150  # Input image dimensions\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "\n",
    "# Load and preprocess your dataset here\n",
    "# Replace the following with your image loading and preprocessing code\n",
    "# Example: Load a single image and preprocess it\n",
    "image_path = \"path/to/your/image.jpg\"\n",
    "image = Image.open(image_path).resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "image = np.array(image) / 255.0\n",
    "image = np.expand_dims(image, axis=0)\n",
    "\n",
    "# Load ViT Feature Extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Load the pre-trained Vision Transformer model\n",
    "base_model = TFViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Add custom classification head\n",
    "inputs = tf.keras.Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "input_ids = feature_extractor(images=inputs)['pixel_values']\n",
    "outputs = base_model(input_ids)['logits']\n",
    "outputs = Flatten()(outputs)\n",
    "outputs = Dense(1024, activation='relu')(outputs)\n",
    "outputs = Dense(512, activation='relu')(outputs)\n",
    "outputs = Dense(256, activation='relu')(outputs)\n",
    "outputs = Dense(128, activation='relu')(outputs)\n",
    "outputs = Dense(NUM_CLASSES, activation='softmax')(outputs)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create data generators for training and validation\n",
    "TRAINING_DIR = r\"C:/Users/Lenovo/Desktop/koi bhi/train/\"\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAINING_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "VALIDATION_DIR = r\"C:/Users/Lenovo/Desktop/koi bhi/test/\"\n",
    "validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    VALIDATION_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.n // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.n // BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Save the model for later use\n",
    "model.save('vision_transformer_image_classifier.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ea60f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

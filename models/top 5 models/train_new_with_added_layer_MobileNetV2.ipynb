{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f306591f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24855 images belonging to 4 classes.\n",
      "Found 5636 images belonging to 4 classes.\n",
      "Epoch 1/50\n",
      "776/776 [==============================] - 658s 841ms/step - loss: 0.9068 - accuracy: 0.9509 - val_loss: 2.5528 - val_accuracy: 0.6317 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "776/776 [==============================] - 653s 841ms/step - loss: 0.0944 - accuracy: 0.9807 - val_loss: 0.5791 - val_accuracy: 0.8130 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "776/776 [==============================] - 651s 839ms/step - loss: 0.1295 - accuracy: 0.9751 - val_loss: 34.3687 - val_accuracy: 0.2786 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "776/776 [==============================] - 647s 834ms/step - loss: 0.0540 - accuracy: 0.9905 - val_loss: 7.0651 - val_accuracy: 0.5009 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "776/776 [==============================] - 650s 837ms/step - loss: 0.0905 - accuracy: 0.9830 - val_loss: 11.1480 - val_accuracy: 0.4819 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "776/776 [==============================] - 644s 830ms/step - loss: 0.0225 - accuracy: 0.9952 - val_loss: 0.0348 - val_accuracy: 0.9918 - lr: 2.0000e-04\n",
      "Epoch 7/50\n",
      "776/776 [==============================] - 656s 845ms/step - loss: 0.0153 - accuracy: 0.9956 - val_loss: 0.0121 - val_accuracy: 0.9949 - lr: 2.0000e-04\n",
      "Epoch 8/50\n",
      "776/776 [==============================] - 647s 833ms/step - loss: 0.0148 - accuracy: 0.9956 - val_loss: 0.0107 - val_accuracy: 0.9949 - lr: 2.0000e-04\n",
      "Epoch 9/50\n",
      "776/776 [==============================] - 652s 840ms/step - loss: 0.0124 - accuracy: 0.9957 - val_loss: 0.0195 - val_accuracy: 0.9938 - lr: 2.0000e-04\n",
      "Epoch 10/50\n",
      "776/776 [==============================] - 651s 839ms/step - loss: 0.0175 - accuracy: 0.9951 - val_loss: 0.0135 - val_accuracy: 0.9954 - lr: 2.0000e-04\n",
      "Epoch 11/50\n",
      "776/776 [==============================] - 647s 834ms/step - loss: 0.0113 - accuracy: 0.9960 - val_loss: 0.0104 - val_accuracy: 0.9959 - lr: 2.0000e-04\n",
      "Epoch 12/50\n",
      "776/776 [==============================] - 645s 831ms/step - loss: 0.0128 - accuracy: 0.9961 - val_loss: 0.0168 - val_accuracy: 0.9956 - lr: 2.0000e-04\n",
      "Epoch 13/50\n",
      "776/776 [==============================] - 647s 833ms/step - loss: 0.0295 - accuracy: 0.9942 - val_loss: 0.0106 - val_accuracy: 0.9970 - lr: 2.0000e-04\n",
      "Epoch 14/50\n",
      "776/776 [==============================] - 641s 826ms/step - loss: 0.0122 - accuracy: 0.9959 - val_loss: 0.0344 - val_accuracy: 0.9870 - lr: 2.0000e-04\n",
      "Epoch 15/50\n",
      "776/776 [==============================] - 641s 826ms/step - loss: 0.0091 - accuracy: 0.9965 - val_loss: 0.0072 - val_accuracy: 0.9959 - lr: 4.0000e-05\n",
      "Epoch 16/50\n",
      "776/776 [==============================] - 642s 828ms/step - loss: 0.0075 - accuracy: 0.9966 - val_loss: 0.0067 - val_accuracy: 0.9950 - lr: 4.0000e-05\n",
      "Epoch 17/50\n",
      "776/776 [==============================] - 643s 828ms/step - loss: 0.0074 - accuracy: 0.9963 - val_loss: 0.0088 - val_accuracy: 0.9968 - lr: 4.0000e-05\n",
      "Epoch 18/50\n",
      "776/776 [==============================] - 640s 825ms/step - loss: 0.0074 - accuracy: 0.9967 - val_loss: 0.0069 - val_accuracy: 0.9964 - lr: 4.0000e-05\n",
      "Epoch 19/50\n",
      "776/776 [==============================] - 642s 826ms/step - loss: 0.0076 - accuracy: 0.9965 - val_loss: 0.0074 - val_accuracy: 0.9957 - lr: 4.0000e-05\n",
      "Epoch 20/50\n",
      "776/776 [==============================] - 641s 826ms/step - loss: 0.0074 - accuracy: 0.9967 - val_loss: 0.0062 - val_accuracy: 0.9961 - lr: 8.0000e-06\n",
      "Epoch 21/50\n",
      "776/776 [==============================] - 643s 828ms/step - loss: 0.0073 - accuracy: 0.9954 - val_loss: 0.0061 - val_accuracy: 0.9961 - lr: 8.0000e-06\n",
      "Epoch 22/50\n",
      "776/776 [==============================] - 641s 825ms/step - loss: 0.0069 - accuracy: 0.9965 - val_loss: 0.0060 - val_accuracy: 0.9961 - lr: 8.0000e-06\n",
      "Epoch 23/50\n",
      "776/776 [==============================] - 639s 823ms/step - loss: 0.0066 - accuracy: 0.9965 - val_loss: 0.0060 - val_accuracy: 0.9961 - lr: 8.0000e-06\n",
      "Epoch 24/50\n",
      "776/776 [==============================] - 636s 820ms/step - loss: 0.0070 - accuracy: 0.9962 - val_loss: 0.0060 - val_accuracy: 0.9961 - lr: 8.0000e-06\n",
      "Epoch 25/50\n",
      "776/776 [==============================] - 636s 820ms/step - loss: 0.0070 - accuracy: 0.9963 - val_loss: 0.0060 - val_accuracy: 0.9961 - lr: 8.0000e-06\n",
      "Epoch 26/50\n",
      "776/776 [==============================] - 651s 838ms/step - loss: 0.0065 - accuracy: 0.9966 - val_loss: 0.0058 - val_accuracy: 0.9961 - lr: 8.0000e-06\n",
      "Epoch 27/50\n",
      "776/776 [==============================] - 651s 838ms/step - loss: 0.0066 - accuracy: 0.9965 - val_loss: 0.0059 - val_accuracy: 0.9959 - lr: 8.0000e-06\n",
      "Epoch 28/50\n",
      "776/776 [==============================] - 650s 837ms/step - loss: 0.0067 - accuracy: 0.9960 - val_loss: 0.0059 - val_accuracy: 0.9956 - lr: 8.0000e-06\n",
      "Epoch 29/50\n",
      "776/776 [==============================] - 651s 839ms/step - loss: 0.0069 - accuracy: 0.9960 - val_loss: 0.0059 - val_accuracy: 0.9954 - lr: 8.0000e-06\n",
      "Epoch 30/50\n",
      "776/776 [==============================] - 659s 848ms/step - loss: 0.0063 - accuracy: 0.9967 - val_loss: 0.0059 - val_accuracy: 0.9954 - lr: 1.6000e-06\n",
      "Epoch 31/50\n",
      "776/776 [==============================] - 654s 842ms/step - loss: 0.0065 - accuracy: 0.9966 - val_loss: 0.0059 - val_accuracy: 0.9954 - lr: 1.6000e-06\n",
      "Epoch 32/50\n",
      "776/776 [==============================] - 653s 842ms/step - loss: 0.0065 - accuracy: 0.9965 - val_loss: 0.0059 - val_accuracy: 0.9945 - lr: 1.6000e-06\n",
      "Epoch 33/50\n",
      "776/776 [==============================] - 652s 840ms/step - loss: 0.0065 - accuracy: 0.9960 - val_loss: 0.0059 - val_accuracy: 0.9943 - lr: 1.0000e-06\n",
      "Epoch 34/50\n",
      "776/776 [==============================] - 653s 841ms/step - loss: 0.0064 - accuracy: 0.9966 - val_loss: 0.0059 - val_accuracy: 0.9943 - lr: 1.0000e-06\n",
      "Epoch 35/50\n",
      "776/776 [==============================] - 651s 839ms/step - loss: 0.0063 - accuracy: 0.9967 - val_loss: 0.0059 - val_accuracy: 0.9943 - lr: 1.0000e-06\n",
      "Epoch 36/50\n",
      "776/776 [==============================] - 651s 839ms/step - loss: 0.0066 - accuracy: 0.9962 - val_loss: 0.0059 - val_accuracy: 0.9943 - lr: 1.0000e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adarsh/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Define constants\n",
    "NUM_CLASSES = 4  # Number of classes (correct, partially correct, incorrect, none)\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224  # Input image dimensions for MobileNetV2\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "# Load MobileNetV2 pre-trained model (include_top=False to exclude the classification layers)\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "\n",
    "# Unfreeze the last few layers of the base model for fine-tuning\n",
    "for layer in base_model.layers[-30:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Add custom classification layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = Dropout(0.5)(x)  # Adjusted dropout rate\n",
    "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Implement a learning rate schedule\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create data generators for training and validation with data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    \"/home/adarsh/Desktop/yoga/train/\",\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    \"/home/adarsh/Desktop/yoga/test/\",\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Define early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping and learning rate schedule\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.n // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.n // BATCH_SIZE,\n",
    "    callbacks=[early_stop, lr_schedule]  # Add early stopping and LR schedule to the callbacks\n",
    ")\n",
    "\n",
    "# Save the model for later use\n",
    "model.save('pose_classification_model_mobilenetv2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3363c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

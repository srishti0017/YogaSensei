{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2734ae2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 00:56:40.453322: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-24 00:56:40.501402: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-24 00:56:40.502361: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-24 00:56:41.702427: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-09-24 00:56:43.382436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-24 00:56:43.430534: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24855 images belonging to 4 classes.\n",
      "Found 5636 images belonging to 4 classes.\n",
      "Epoch 1/25\n",
      "776/776 [==============================] - 3419s 4s/step - loss: 1.1968 - accuracy: 0.8989 - val_loss: 2.4530 - val_accuracy: 0.0648 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "776/776 [==============================] - 3431s 4s/step - loss: 0.1492 - accuracy: 0.9652 - val_loss: 0.8939 - val_accuracy: 0.7951 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "776/776 [==============================] - 3434s 4s/step - loss: 0.1294 - accuracy: 0.9695 - val_loss: 0.5230 - val_accuracy: 0.8365 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "776/776 [==============================] - 3432s 4s/step - loss: 0.0823 - accuracy: 0.9813 - val_loss: 0.2111 - val_accuracy: 0.9308 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "776/776 [==============================] - 3448s 4s/step - loss: 0.1160 - accuracy: 0.9733 - val_loss: 0.2855 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "776/776 [==============================] - 3447s 4s/step - loss: 0.0623 - accuracy: 0.9865 - val_loss: 0.3840 - val_accuracy: 0.8420 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "776/776 [==============================] - 3477s 4s/step - loss: 0.0753 - accuracy: 0.9850 - val_loss: 0.2031 - val_accuracy: 0.9272 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "776/776 [==============================] - 3416s 4s/step - loss: 0.0755 - accuracy: 0.9838 - val_loss: 0.0536 - val_accuracy: 0.9911 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "776/776 [==============================] - 3426s 4s/step - loss: 0.0403 - accuracy: 0.9912 - val_loss: 0.0529 - val_accuracy: 0.9865 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "776/776 [==============================] - 3472s 4s/step - loss: 0.0633 - accuracy: 0.9867 - val_loss: 0.2355 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "776/776 [==============================] - 3434s 4s/step - loss: 0.0423 - accuracy: 0.9911 - val_loss: 0.4171 - val_accuracy: 0.8984 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "776/776 [==============================] - 3439s 4s/step - loss: 0.0524 - accuracy: 0.9900 - val_loss: 0.0916 - val_accuracy: 0.9853 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "776/776 [==============================] - 3425s 4s/step - loss: 0.0182 - accuracy: 0.9951 - val_loss: 0.0113 - val_accuracy: 0.9956 - lr: 2.0000e-04\n",
      "Epoch 14/25\n",
      "776/776 [==============================] - 3347s 4s/step - loss: 0.0115 - accuracy: 0.9961 - val_loss: 0.0092 - val_accuracy: 0.9959 - lr: 2.0000e-04\n",
      "Epoch 15/25\n",
      "776/776 [==============================] - 3368s 4s/step - loss: 0.0106 - accuracy: 0.9958 - val_loss: 0.0080 - val_accuracy: 0.9964 - lr: 2.0000e-04\n",
      "Epoch 16/25\n",
      "776/776 [==============================] - 3368s 4s/step - loss: 0.0105 - accuracy: 0.9961 - val_loss: 0.0094 - val_accuracy: 0.9950 - lr: 2.0000e-04\n",
      "Epoch 17/25\n",
      "776/776 [==============================] - 3319s 4s/step - loss: 0.0103 - accuracy: 0.9958 - val_loss: 0.0915 - val_accuracy: 0.9712 - lr: 2.0000e-04\n",
      "Epoch 18/25\n",
      "776/776 [==============================] - 3306s 4s/step - loss: 0.0117 - accuracy: 0.9963 - val_loss: 0.0172 - val_accuracy: 0.9931 - lr: 2.0000e-04\n",
      "Epoch 19/25\n",
      "776/776 [==============================] - 3323s 4s/step - loss: 0.0087 - accuracy: 0.9963 - val_loss: 0.0080 - val_accuracy: 0.9961 - lr: 4.0000e-05\n",
      "Epoch 20/25\n",
      "776/776 [==============================] - 3323s 4s/step - loss: 0.0080 - accuracy: 0.9963 - val_loss: 0.0073 - val_accuracy: 0.9964 - lr: 4.0000e-05\n",
      "Epoch 21/25\n",
      "776/776 [==============================] - 3312s 4s/step - loss: 0.0074 - accuracy: 0.9965 - val_loss: 0.0071 - val_accuracy: 0.9966 - lr: 4.0000e-05\n",
      "Epoch 22/25\n",
      "776/776 [==============================] - 3273s 4s/step - loss: 0.0071 - accuracy: 0.9969 - val_loss: 0.0066 - val_accuracy: 0.9959 - lr: 4.0000e-05\n",
      "Epoch 23/25\n",
      "776/776 [==============================] - 3305s 4s/step - loss: 0.0072 - accuracy: 0.9961 - val_loss: 0.0064 - val_accuracy: 0.9966 - lr: 4.0000e-05\n",
      "Epoch 24/25\n",
      "776/776 [==============================] - 3284s 4s/step - loss: 0.0072 - accuracy: 0.9965 - val_loss: 0.0066 - val_accuracy: 0.9963 - lr: 4.0000e-05\n",
      "Epoch 25/25\n",
      "776/776 [==============================] - 3324s 4s/step - loss: 0.0069 - accuracy: 0.9963 - val_loss: 0.0069 - val_accuracy: 0.9964 - lr: 4.0000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adarsh/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Define constants\n",
    "NUM_CLASSES = 4  # Number of classes (correct, partially correct, incorrect, none)\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224  # Input image dimensions for ResNet50\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "\n",
    "# Load ResNet50 pre-trained model (include_top=False to exclude the classification layers)\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "\n",
    "# Flatten the output from the base model\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = Dropout(0.5)(x)  # Adjusted dropout rate\n",
    "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Implement a learning rate schedule\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create data generators for training and validation with data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/home/adarsh/Desktop/yoga/train/',\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    '/home/adarsh/Desktop/yoga/test/',\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Define early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping and learning rate schedule\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.n // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.n // BATCH_SIZE,\n",
    "    callbacks=[early_stop, lr_schedule]  # Add early stopping and LR schedule to the callbacks\n",
    ")\n",
    "\n",
    "# Save the model for later use\n",
    "model.save('pose_classification_model_resnet50.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5513032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
